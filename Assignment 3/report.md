# CIFAR 100 Dataset

### 2017CS10350 & 2017CS10347

## Week 1
We have implemented the model we had previosuly used with the CIFAR10 dataset which was using only CNN layers and a dense layer. We have increased the numbers of CNNs and also the filter size for the purpose of this assignment. We have used data augmentation too but the improvement in accuracy has plateud after 50 iterations and even though we trained the model for 300 iterations, there weren't any significant improvement and the max accuracy achieved on the test data was about 60+%. We have used different validation split from train data every 50 epochs for generalization to unseen datas. Next week we plan to change our model and experiment using some new techniques.

## Week 2
We have experimented with many different combination of convolution layers, dense layer, dropouts etc. but nothing significant can be achived through these chznges. We have gained a littel accuracy but we have figured out thta increasing the coomplexity by adding layers will not make our model improve more significantly and hench we are reading about EfficentNet and the mobile net structure and are going to implement it from here on.

## Week 3
We read about a few models such as VGG16 and tried it but the accuracy didn't improve so we decided to stay with our previous submission

## Week 4
This week we tried a resnet model with data augmentation and the accuracy has improved from the last time. The best accuracy we could achieve after 4hours of training is 66% which is a lot better than our previous submission. Next time we are planning to test a deeper resnet model and also add random dropout and some other callbacks and test tuning other hyper parameters too. We are also planning to try efficientnet models. We found some code but were unable to run it as they were for older tensorflow models so we would try upgrading the code ourselves for the newer version.

## Week 5
This week we have tried with Resnet and its varient. We have tried by varying different depth and filter numbers. We have also tried with including pooling layers in between. We have also tried with data augmentation which include rotation, scaling, feature centering and other feature of ImageDataGeneration from keras. Although the training accuracy was increasing but the test data accuracy was not showing much improvement. Ex. After running one resnet with depth 47 resnet with max pooling layer for 40 epochs the train data accuracy was observed at 52% but the test data accuracy was sill at 11%.  Currently we have trained resnet110 which had reached test accuracy of 68%. We trained the model on our local machine with a GTX1050 but we are hoping that if we run the same model on the HPC, the number of epochs and hence the accuracy may increase.

## Week 6
This week we have tried a lot of different approaches. First we tried to improve the resnet by varying parameters but we observed that it was taking a lot of time in training and probably we can't have that much time. We have thought of distributing the data and then training the data set on the coarse set first and then on fine. We have thought that since the data set had a course data set and then fin i.e. there is a sub-category of the data. There are 20 such sub-category like aquatic animals, mamals, cars, etc.. And within these sub-category there are 5 category each. We have though that since one sub-category with have similar feature in image hence if we train models on these sets only then they can learn better feature in respect to those categories. Also we need to first train a model to figure out which sub-category this model belongs to and then predict on that given category. We have build the articheture but we have found that it was giving a very low accuracy. After training the sub-category predictor for 100 epochs and training respective fine predictor for 50 epoch the test accracy obtained was around 15%. On thinking again we realized that we need to have the Course predictor to have a very high accuracy. SInce if it predicts wrong then definetly the fine one is going to predict it wrongly. But we weren;t ab;e to get that accuracy. We have shifted to the old architecture again with some fine tuning with simple single model.
